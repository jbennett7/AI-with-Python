{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e47a78e",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecfda1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "input_data = np.array([[5.1, -2.9, 3.3],\n",
    "                       [-1.2, 7.8, -6.1],\n",
    "                       [3.9, 0.4, 2.1],\n",
    "                       [7.3, -9.9, -4.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb61f50",
   "metadata": {},
   "source": [
    "## Binarization\n",
    "Convert values into boolean values depending if the data is below or above a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766da9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Binarized data:\n",
      " [[1. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "data_binarized = preprocessing.Binarizer(threshold=2.1).transform(input_data)\n",
    "print(\"\\nBinarized data:\\n\", data_binarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1652f5",
   "metadata": {},
   "source": [
    "## Mean removal\n",
    "A method to remove bias from features is to remove the mean from the features so that each feature is centered around zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99d3ca26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEFORE:\n",
      "Mean = [ 3.775 -1.15  -1.3  ]\n",
      "Std deviation = [3.12039661 6.36651396 4.0620192 ]\n"
     ]
    }
   ],
   "source": [
    "# Print mean and standard deviation\n",
    "print(\"\\nBEFORE:\")\n",
    "print(\"Mean =\", input_data.mean(axis=0))\n",
    "print(\"Std deviation =\", input_data.std(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a355b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFTER:\n",
      "Mean = [1.11022302e-16 0.00000000e+00 2.77555756e-17]\n",
      "Std deviation = [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Remove mean\n",
    "data_scaled = preprocessing.scale(input_data)\n",
    "print(\"\\nAFTER:\")\n",
    "print(\"Mean =\", data_scaled.mean(axis=0))\n",
    "print(\"Std deviation =\", data_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b172f2",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "In our feature vector, the value of each feature can vary between many random values. So it becomes important to scale those features so that it is a level playing field for the machine learning algorithm to train on. We don't want any feature to be artificially large or small just because of the nature of the measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47a9849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Min max scaled data:\n",
      " [[0.74117647 0.39548023 1.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.6        0.5819209  0.87234043]\n",
      " [1.         0.         0.17021277]]\n"
     ]
    }
   ],
   "source": [
    "# Min max scaling\n",
    "data_scaler_minmax = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled_minmax = data_scaler_minmax.fit_transform(input_data)\n",
    "print(\"\\nMin max scaled data:\\n\", data_scaled_minmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969e679",
   "metadata": {},
   "source": [
    "Each row is scaled so that the maximum value is $1$ and all the other values are relative to this value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b8d82",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "We use the process of normalization to modify the values in the feature vector so that we can measure them on a common scale. In machine learning, we use many different forms of normalization. SOme of the most common forms of normalization aim to modify the values so that they sum up to $1$. __L1 normalization__, which refers to __Least Absolute Deviations__, works by making sure that the sum of absolute values is $1$ in each row. __L2 normalization__, which refers to least squares, works by making sure that the sum of squares is $1$.\n",
    "\n",
    "In general, L1 normalization technique is considered more robust than L2 normalization technique. L1 normalization technique is robust because it is resistant to outliers in the data. A lot of times, data tends to contain outliers and we cannot do anything about it. We want to use techniques that can safely and effectively ignore them during the calculations. If we are solving a problem where outliers are important, then maybe L2 normalization becomes a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5426c066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L1 normalized data:\n",
      " [[ 0.45132743 -0.25663717  0.2920354 ]\n",
      " [-0.0794702   0.51655629 -0.40397351]\n",
      " [ 0.609375    0.0625      0.328125  ]\n",
      " [ 0.33640553 -0.4562212  -0.20737327]]\n",
      "\n",
      "L2 normalized data:\n",
      " [[ 0.75765788 -0.43082507  0.49024922]\n",
      " [-0.12030718  0.78199664 -0.61156148]\n",
      " [ 0.87690281  0.08993875  0.47217844]\n",
      " [ 0.55734935 -0.75585734 -0.34357152]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data\n",
    "data_normalized_l1 = preprocessing.normalize(input_data, norm='l1')\n",
    "data_normalized_l2 = preprocessing.normalize(input_data, norm='l2')\n",
    "print(\"\\nL1 normalized data:\\n\", data_normalized_l1)\n",
    "print(\"\\nL2 normalized data:\\n\", data_normalized_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052df4fd",
   "metadata": {},
   "source": [
    "## Label encoding\n",
    "When we perform classification, we usually deal with a lot of labels. These labels can be in the form of words, numbers, or something else. The machine learning functions in __sklearn__ expect them to be numbers. So if they are already numbers, then we can use them directly to start training. But this is not usually the case.\n",
    "\n",
    "In the ral world, labels are in the form of words, because words are human readable. We label our training data with words so that the mapping can be tracked. To convert word labels into numbers, we need to use a label encoder. Label encoding refers to the process of transforming the word labels into numerical form. This enables the algorithms to operate on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ae22ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label mapping:\n",
      "black --> 0\n",
      "green --> 1\n",
      "red --> 2\n",
      "white --> 3\n",
      "yellow --> 4\n",
      "\n",
      "Labels = ['green', 'red', 'black']\n",
      "Encoded values = [1, 2, 0]\n",
      "\n",
      "Encoded values = [3, 0, 4, 1]\n",
      "Decoded labels = ['white', 'black', 'yellow', 'green']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Sample input labels\n",
    "input_labels = ['red', 'black', 'red', 'green', 'black', 'yellow', 'white']\n",
    "\n",
    "# Create label encoder and fit the labels\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "encoder.fit(input_labels)\n",
    "\n",
    "# Print the mapping\n",
    "print(\"\\nLabel mapping:\")\n",
    "for i, item in enumerate(encoder.classes_):\n",
    "    print(item, '-->', i)\n",
    "\n",
    "# Encode a set of labels using the encoder\n",
    "test_labels = ['green', 'red', 'black']\n",
    "encoded_values = encoder.transform(test_labels)\n",
    "print(\"\\nLabels =\", test_labels)\n",
    "print(\"Encoded values =\", list(encoded_values))\n",
    "\n",
    "# Decode a set of values using the encoder\n",
    "encoded_values = [3, 0, 4, 1]\n",
    "decoded_list = encoder.inverse_transform(encoded_values)\n",
    "print(\"\\nEncoded values =\", encoded_values)\n",
    "print(\"Decoded labels =\", list(decoded_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit",
   "language": "python",
   "name": "scikit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
